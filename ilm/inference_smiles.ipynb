{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('D:\\\\My Documents\\\\projects\\\\drug\\\\')\n",
    "custom_vocab_fp='D:\\\\My Documents\\\\projects\\\\drug\\\\st\\smiles_transformer\\data\\chembl_vocab.csv'\n",
    "from nanoGPT.model import GPTConfig, GPT\n",
    "from st.smiles_transformer.utils import split as split_method\n",
    "from rdkit import Chem\n",
    "\n",
    "MODEL_DIR = 'train_smiles'\n",
    "MASK_CLS = 'ilm.mask.custom.MaskSmiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 4.74M\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "block_size = 256 # context of up to 256 previous characters\n",
    "n_layer = 5 # 6\n",
    "n_head = 8\n",
    "n_embd = 128 # 256\n",
    "dropout = 0.2\n",
    "bias = False\n",
    "vocab_size=53 \n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                bias=bias, vocab_size=vocab_size, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "#model.load_state_dict(torch.load('D:\\\\My Documents\\\\projects\\\\drug\\\\ilm\\\\train_smiles\\\\model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (['<pad>', '<unk>', '<eos>', '<sos>', '<mask>', 'c', 'C', '(', ')', 'O', '1', '2', '=', 'N', '@', '[', ']', 'n', '3', 'H', '4', 'F', '-', 'S', 'Cl', '/', 's', 'o', '5', '+', '.', '#', 'Br', '\\\\', 'P', '6', 'I', '7', 'Na', '-2', '-3', '8', 'B', '9', 'Si', '%10', 'K', '%11', '-4', 'Se', '<|startofinfill|>', '<|endofinfill|>', '<|infill_element|>', '<|startofinfill|>', '<|endofinfill|>', '<|infill_element|>'], {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3, '<mask>': 4, 'c': 5, 'C': 6, '(': 7, ')': 8, 'O': 9, '1': 10, '2': 11, '=': 12, 'N': 13, '@': 14, '[': 15, ']': 16, 'n': 17, '3': 18, 'H': 19, '4': 20, 'F': 21, '-': 22, 'S': 23, 'Cl': 24, '/': 25, 's': 26, 'o': 27, '5': 28, '+': 29, '.': 30, '#': 31, 'Br': 32, '\\\\': 33, 'P': 34, '6': 35, 'I': 36, '7': 37, 'Na': 38, '-2': 39, '-3': 40, '8': 41, 'B': 42, '9': 43, 'Si': 44, '%10': 45, 'K': 46, '%11': 47, '-4': 48, 'Se': 49, '<|startofinfill|>': 50, '<|endofinfill|>': 51, '<|infill_element|>': 52}) {50: '<|startofinfill|>', 51: '<|endofinfill|>', 52: '<|infill_element|>'}\n",
      "['<pad>', '<unk>', '<eos>', '<sos>', '<mask>', 'c', 'C', '(', ')', 'O', '1', '2', '=', 'N', '@', '[', ']', 'n', '3', 'H', '4', 'F', '-', 'S', 'Cl', '/', 's', 'o', '5', '+', '.', '#', 'Br', '\\\\', 'P', '6', 'I', '7', 'Na', '-2', '-3', '8', 'B', '9', 'Si', '%10', 'K', '%11', '-4', 'Se', '<|startofinfill|>', '<|endofinfill|>', '<|infill_element|>', '<|startofinfill|>', '<|endofinfill|>', '<|infill_element|>', '<|startofinfill|>', '<|endofinfill|>', '<|infill_element|>']\n",
      "{'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3, '<mask>': 4, 'c': 5, 'C': 6, '(': 7, ')': 8, 'O': 9, '1': 10, '2': 11, '=': 12, 'N': 13, '@': 14, '[': 15, ']': 16, 'n': 17, '3': 18, 'H': 19, '4': 20, 'F': 21, '-': 22, 'S': 23, 'Cl': 24, '/': 25, 's': 26, 'o': 27, '5': 28, '+': 29, '.': 30, '#': 31, 'Br': 32, '\\\\': 33, 'P': 34, '6': 35, 'I': 36, '7': 37, 'Na': 38, '-2': 39, '-3': 40, '8': 41, 'B': 42, '9': 43, 'Si': 44, '%10': 45, 'K': 46, '%11': 47, '-4': 48, 'Se': 49, '<|startofinfill|>': 50, '<|endofinfill|>': 51, '<|infill_element|>': 52}\n",
      "dict_values(['<|startofinfill|>', '<|endofinfill|>', '<|infill_element|>'])\n",
      "dict_keys([50, 51, 52])\n",
      "{'<|startofinfill|>': 50, '<|endofinfill|>': 51, '<|infill_element|>': 52}\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import ilm.tokenize_util\n",
    "tokenizer = ilm.tokenize_util.Tokenizer['CUSTOM']\n",
    "ilm.tokenize_util.set_custom_vocab_fp(custom_vocab_fp)\n",
    "#tokenizer = ilm.tokenize_util.Tokenizer.GPT2\n",
    "with open(os.path.join(MODEL_DIR, 'additional_ids_to_tokens.pkl'), 'rb') as f:\n",
    "    additional_ids_to_tokens = pickle.load(f)\n",
    "additional_tokens_to_ids = {v:k for k, v in additional_ids_to_tokens.items()}\n",
    "try:\n",
    "    ilm.tokenize_util.update_tokenizer(additional_ids_to_tokens, tokenizer)\n",
    "except ValueError:\n",
    "    print('Already updated')\n",
    "print(additional_tokens_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=NC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=N5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=N5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=S5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=S5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=S5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=N5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "Before ref COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\n",
      "['COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=NC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6', 'COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=N5)C3=O)C6=CC=NC=C6', 'COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=S5)C3=O)C6=CC=NC=C6']\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# Create context\n",
    "\n",
    "context = \"COC1=CC=C(C=C1)N2C(=NN=C2SCCCN3C(=O)C4=CC=CC5=C4C(=CC=C5)C3=O)C6=CC=NC=C6\".strip()\n",
    "ref = context\n",
    "\n",
    "print(\"Input\", context)\n",
    "context = split_method(context.strip()).split()\n",
    "#print(context)\n",
    "L =len(context)\n",
    "aList = list(range(L))\n",
    "import random\n",
    "from ilm.infer_smiles import infill_with_ilm\n",
    "new_drug_dict=[]\n",
    "num_new_samples = 0\n",
    "ref_sampled = 0\n",
    "for i in range(100):\n",
    "    sampled_list = random.sample(aList, 2)\n",
    "    # Change just one location\n",
    "    for pos in sampled_list:\n",
    "        context[pos]='<mask>'\n",
    "\n",
    "    context_ids = ilm.tokenize_util.encode(context, tokenizer)\n",
    "    # Replace blanks with appropriate tokens from left to right\n",
    "    _blank_id = ilm.tokenize_util.encode(['<mask>'], tokenizer)[0]\n",
    "    #print(_blank_id)\n",
    "    for pos in sampled_list:\n",
    "        context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_element|>']\n",
    "    output = ilm.tokenize_util.decode(context_ids, tokenizer)\n",
    "    r = ''.join(output.strip())\n",
    "    #print(\"Input to model\", r)\n",
    "    # Temperature > 1 introduces more randomness\n",
    "    generated = infill_with_ilm(\n",
    "        model,\n",
    "        additional_tokens_to_ids,\n",
    "        context_ids,\n",
    "        num_infills=32,\n",
    "        nucleus = 1,\n",
    "        temperature=1.5)\n",
    "    for g in generated:\n",
    "        output = ilm.tokenize_util.decode(g, tokenizer)\n",
    "        r = ''.join(output.strip())\n",
    "        #print(i, r)\n",
    "        m = Chem.MolFromSmiles(r)\n",
    "        if m is not None:\n",
    "            print(\"Before ref\", r)\n",
    "            if r != ref:\n",
    "               if r not in new_drug_dict:\n",
    "                   new_drug_dict.append(r)\n",
    "                   num_new_samples+=1\n",
    "            else:\n",
    "               ref_sampled +=1\n",
    "print(new_drug_dict)\n",
    "print(ref_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ilm.infer_smiles import infill_with_ilm\n",
    "\n",
    "generated = infill_with_ilm(\n",
    "    model,\n",
    "    additional_tokens_to_ids,\n",
    "    context_ids,\n",
    "    num_infills=64,\n",
    "    nucleus = 1)\n",
    "#for g in generated:\n",
    "#    print('-' * 80)\n",
    "#    print(ilm.tokenize_util.decode(g, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "emb = nn.Embedding(8,16)\n",
    "inp = torch.Tensor([[1, 2 ,3], [4,5,6]])\n",
    "print(inp.size())\n",
    "out = emb(inp.long())\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function. No batch support?\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model) # (T,H)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TrfmSeq2seq(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, nhead=8, n_layers=4, batch_first=True, dropout=0.1):\n",
    "        super(TrfmSeq2seq, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(in_size, hidden_size)\n",
    "        self.pe = PositionalEncoding(hidden_size, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead, batch_first=batch_first,dropout=dropout)\n",
    "        self.trfm = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (B, T)\n",
    "        embedded = self.embed(src)  # (B,T, H)\n",
    "        embedded = self.pe(embedded) # (B,T, H)\n",
    "        out = self.trfm(embedded) # (B,T, H)\n",
    "        print(src.size(), out.size())\n",
    "        return out # (B,T,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 3, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6497,  0.1046, -1.1334, -0.8931,  1.6584, -0.3894, -0.7980,\n",
       "          -0.1967,  1.8961, -0.5315,  1.5660,  0.7552, -0.7275,  0.3149,\n",
       "          -0.1863,  0.2101],\n",
       "         [-0.7765,  0.9591, -0.2496,  0.6830, -0.1514, -0.3222,  0.4608,\n",
       "          -0.6958, -1.1900,  2.8837,  0.7874,  0.1602, -1.3347, -0.1818,\n",
       "          -0.0690, -0.9632],\n",
       "         [-0.5403, -0.0924, -1.1236,  1.0544,  1.3352, -0.5160, -0.2795,\n",
       "          -1.8338,  1.5228, -0.4185,  0.6669,  1.0725, -1.6530,  0.9955,\n",
       "          -0.1437, -0.0463]],\n",
       "\n",
       "        [[-0.2631, -1.7040, -0.4537, -0.1999,  1.7409,  1.0370, -0.9161,\n",
       "          -0.1871, -0.3922, -0.9199,  2.1893,  0.0817, -0.3573, -0.7940,\n",
       "           1.1147,  0.0236],\n",
       "         [-1.1230, -0.0374, -0.2310,  0.3391,  2.3228, -1.3117, -0.1088,\n",
       "          -0.9919,  0.2727,  0.3949,  2.2534,  0.1860, -0.3998, -0.3177,\n",
       "          -0.4291, -0.8185],\n",
       "         [-0.4133, -0.6562, -0.0973, -1.0399,  2.5576, -0.4564, -1.3055,\n",
       "           0.0814,  0.8512, -0.1656,  1.7037,  0.8820,  0.0384, -0.5805,\n",
       "          -0.4802, -0.9195]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = TrfmSeq2seq(8, 16)\n",
    "inp = torch.Tensor([[1, 2 ,3], [4,5,6]]).long()\n",
    "m(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
